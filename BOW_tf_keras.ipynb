{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0534c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131102e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89f73d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-49996171cb79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Output : Predict the centerged/targeted word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# train a CBOW model using Keras and tf to learn embeddings and predict a target word given its context words\n",
    "# Input : a set of surrounding context words\n",
    "# Output : Predict the centerged/targeted word\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87546d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step -1 : Sample Text\n",
    "text = \"\"\"Neural networks power modern deep learning systems.\n",
    "          Machine Learning is a subset of Artificial Intelligence.\n",
    "          Natural language processing allows computers to understand human language.\n",
    "          Convolutional networks are widely used in computer vision tasks.\n",
    "          Transformers have changed the landscape of NLP and AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2 : Preprocessing\n",
    "text = text.lower().replace(\".\", \"\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d99970",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c1e19d45895e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# Step-3 : Generate CBOW (context -> target) training data\n",
    "# For every target word, we take 2 words before and after , each, as context\n",
    "# x = list of context word indices, y=OHE target word index\n",
    "window_size = 2\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(window_size, len(words) - window_size):\n",
    "    context = [words[j] for j in range(i - window_size, i+window_size+1) if j != i]\n",
    "    target = words[i]\n",
    "    context_ids = [word_index[w] for w in context]\n",
    "    target_id = word_index[target]\n",
    "    X.append(context_ids)\n",
    "    y.append(to_categorical(target_id, num_classes=vocab_size))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f766fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
