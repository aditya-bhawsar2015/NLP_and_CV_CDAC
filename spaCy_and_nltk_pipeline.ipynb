{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a921ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline in NLTK (manual work , no direct support)\n",
    "import nltk \n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize, ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa24818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2473a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== NLTK NLP Pipeline ========\n",
      "Sentences ( 2): ['Apple was founded by Steve Jobs in California.', 'He emailed support@apple.com from L.A.']\n",
      "\n",
      " Tokens (17):  ['Apple', 'was', 'founded', 'by', 'Steve', 'Jobs', 'in', 'California', '.', 'He', 'emailed', 'support', '@', 'apple.com', 'from', 'L.A', '.']\n",
      "\n",
      " POS Tags : [('Apple', 'NNP'), ('was', 'VBD'), ('founded', 'VBN'), ('by', 'IN'), ('Steve', 'NNP'), ('Jobs', 'NNP'), ('in', 'IN'), ('California', 'NNP'), ('.', '.'), ('He', 'PRP'), ('emailed', 'VBD'), ('support', 'NN'), ('@', 'NNP'), ('apple.com', 'NN'), ('from', 'IN'), ('L.A', 'NNP'), ('.', '.')]\n",
      "\n",
      " Named Entities:  [('Apple', 'PERSON'), ('Steve Jobs', 'PERSON'), ('California', 'GPE')]\n",
      "\n",
      "Pipeline Stages : ['Tokenizer', 'POS Tagger', 'Chunker/NER'] \n"
     ]
    }
   ],
   "source": [
    "# Sample text \n",
    "text = \"Apple was founded by Steve Jobs in California. He emailed support@apple.com from L.A.\"\n",
    "\n",
    "# 1. Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# 2. Word Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 3. POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# 4. Named Entity Recognition\n",
    "ne_tree = ne_chunk(pos_tags)\n",
    "\n",
    "# Extract Named Entities in (entity, type) form\n",
    "named_entities = []\n",
    "for subtree in ne_tree:\n",
    "    if isinstance(subtree, Tree):\n",
    "        entity = \" \".join(token for token, pos in subtree.leaves())\n",
    "        label = subtree.label()\n",
    "        named_entities.append((entity, label))\n",
    "\n",
    "# Output Summary\n",
    "print(\"====== NLTK NLP Pipeline ========\")\n",
    "print(f\"Sentences ( {len(sentences)}):\", sentences)\n",
    "print(f\"\\n Tokens ({len(tokens)}): \", tokens)\n",
    "print(f\"\\n POS Tags :\", pos_tags)\n",
    "print(f\"\\n Named Entities: \", named_entities)\n",
    "print(f\"\\nPipeline Stages : ['Tokenizer', 'POS Tagger', 'Chunker/NER'] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aed244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496aab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components  :  [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001FEEA9D1250>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001FEEA9D1370>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001FEE4B40F90>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001FEE6D86D10>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001FEE6D71290>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001FEE4B417E0>)]\n",
      "Component names :  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Sentencizer added.\n",
      "\n",
      " Tokens : \n",
      "['Apple', 'was', 'founded', 'by', 'Steve', 'Jobs', 'in', 'California', '.', 'He', 'emailed', 'support@apple.com', 'from', 'L.A.']\n",
      "\n",
      " Sentences : \n",
      "['Apple was founded by Steve Jobs in California.', 'He emailed support@apple.com from L.A.']\n",
      "\n",
      "Explanations:\n",
      "VBZ: verb, 3rd person singular present\n",
      "nsubj: nominal subject\n",
      "ORG: Companies, agencies, institutions, etc.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (d:\\workspace\\NLP_CV\\.venv\\Lib\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mORG:\u001b[39m\u001b[33m\"\u001b[39m, explain(\u001b[33m\"\u001b[39m\u001b[33mORG\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Visualise named entities (works best in Jupyter or Colab)\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# displacy.serve(doc, style=\"ent\", host=\"127.0.0.1\", port = 5000)   # For non- Jupyter/Colab\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# For Jupyter/Colab\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\workspace\\NLP_CV\\.venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (d:\\workspace\\NLP_CV\\.venv\\Lib\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "#### spaCy Pipeline\n",
    "import spacy\n",
    "from spacy import displacy, explain\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(\"Pipeline components  : \", nlp.pipeline)\n",
    "print(\"Component names : \", nlp.pipe_names)\n",
    "\n",
    "# Add sentencizer if not present\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe('sentencizer', before=\"parser\")\n",
    "    print(\"Sentencizer added.\")\n",
    "\n",
    "# Process data\n",
    "doc = nlp(text)    \n",
    "\n",
    "# Print tokens \n",
    "print(\"\\n Tokens : \")\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Print Sentences :\n",
    "print(\"\\n Sentences : \")\n",
    "print([sent.text for sent in doc.sents])\n",
    "\n",
    "# Explain some POS/DEP/NER tags :\n",
    "print(\"\\nExplanations:\")\n",
    "print(\"VBZ:\", explain(\"VBZ\"))\n",
    "print(\"nsubj:\", explain(\"nsubj\"))\n",
    "print(\"ORG:\", explain(\"ORG\"))\n",
    "\n",
    "# Visualise named entities (works best in Jupyter or Colab)\n",
    "# displacy.serve(doc, style=\"ent\", host=\"127.0.0.1\", port = 5000)   # For non- Jupyter/Colab\n",
    "\n",
    "displacy.render(doc, style=\"ent\")   # For Jupyter/Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed11129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary : \n",
      "Total components : 7\n",
      "NER present : True\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"\\nFinal Summary : \")\n",
    "print(f\"Total components : {len(nlp.pipe_names)}\")\n",
    "print(f\"NER present : {'ner' in nlp.pipe_names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
