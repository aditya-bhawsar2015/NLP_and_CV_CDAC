{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b174cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import wikipedia\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c6c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-1 : Get text from wikipedia and lowercase\n",
    "text = wikipedia.page(\"Geoffrey Hinton\").content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2: Tokenization\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8aa7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STep-3: Remove punctuations and numbers\n",
    "tokens = [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f2dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-4 : Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30d78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step- 5a: Porter Stemmer:\n",
    "porter = PorterStemmer()\n",
    "porter_stemmed = [porter.stem(word) for word in filtered_tokens]\n",
    "porter_text = ' '.join(porter_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63086f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step- 5b : Snowball Stemmer:\n",
    "snowball = SnowballStemmer('english')\n",
    "snowball_stemmed = [snowball.stem(word) for word in filtered_tokens]\n",
    "snowball_text = ' '.join(snowball_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0867f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STep- 5c: Lemmatization using WordNet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "nltk_lemmatized_text = ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dad5852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Porter Stemmer Result ======\n",
      "geoffrey everest hinton born comput scientist cognit scientist cognit psychologist known work artifici neural network earn titl godfath ai hinton univers professor emeritu univers toronto divid time work googl googl brain univers toronto publicli announc departur googl may cite concern mani risk artifici intellig ai technolog becam chief scientif advisor vector institut toronto david rumelhart ronald william hinton highli cite paper publish popularis backpropag algorithm train neural network although first propos approach hinton view lead figur deep learn commun mileston alexnet design collabor student alex krizhevski ilya sutskev imagenet challeng breakthrough field comput vision hinton receiv ture award often refer nobel prize comput togeth yoshua bengio yann lecun work deep learn sometim refer godfath deep learn continu give public talk togeth also award along john hopfield nobel prize physic foundat discoveri invent enabl machin learn artifici neural network may hinton announc resi \n",
      "\n",
      "====== Snowball Stemmer Result ======\n",
      "geoffrey everest hinton born comput scientist cognit scientist cognit psychologist known work artifici neural network earn titl godfath ai hinton univers professor emeritus univers toronto divid time work googl googl brain univers toronto public announc departur googl may cite concern mani risk artifici intellig ai technolog becam chief scientif advisor vector institut toronto david rumelhart ronald william hinton high cite paper publish popularis backpropag algorithm train neural network although first propos approach hinton view lead figur deep learn communiti mileston alexnet design collabor student alex krizhevski ilya sutskev imagenet challeng breakthrough field comput vision hinton receiv ture award often refer nobel prize comput togeth yoshua bengio yann lecun work deep learn sometim refer godfath deep learn continu give public talk togeth also award along john hopfield nobel prize physic foundat discoveri invent enabl machin learn artifici neural network may hinton announc resi \n",
      "\n",
      "====== NLTK Lemmatizer Result ======\n",
      "geoffrey everest hinton born computer scientist cognitive scientist cognitive psychologist known work artificial neural network earned title godfather ai hinton university professor emeritus university toronto divided time working google google brain university toronto publicly announcing departure google may citing concern many risk artificial intelligence ai technology became chief scientific advisor vector institute toronto david rumelhart ronald williams hinton highly cited paper published popularised backpropagation algorithm training neural network although first propose approach hinton viewed leading figure deep learning community milestone alexnet designed collaboration student alex krizhevsky ilya sutskever imagenet challenge breakthrough field computer vision hinton received turing award often referred nobel prize computing together yoshua bengio yann lecun work deep learning sometimes referred godfather deep learning continued give public talk together also awarded along joh \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Results : \n",
    "print(\"====== Porter Stemmer Result ======\")\n",
    "print(porter_text[:1000], \"\\n\")\n",
    "\n",
    "print(\"====== Snowball Stemmer Result ======\")\n",
    "print(snowball_text[:1000], \"\\n\")\n",
    "\n",
    "print(\"====== NLTK Lemmatizer Result ======\")\n",
    "print(nltk_lemmatized_text[:1000], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5a5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6307b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098e5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STep-1 : Get and lowercase text from wikipedia\n",
    "text = wikipedia.page(\"Geoffrey Hinton\").content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc24636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2 : Process using spacy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9afd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STep-3 remove stopwords, punctuation, and non-alphabetic tokens; lemmatize\n",
    "cleaned_tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fdc7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 : clean the text\n",
    "spacy_cleaned_text = ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b599f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
