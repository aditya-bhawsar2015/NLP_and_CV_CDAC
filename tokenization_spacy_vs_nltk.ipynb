{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db9106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non English tokenization using NLTK and spaCy\n",
    "# Fetch a hindi page from Wikipedia\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9210ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://hi.wikipedia.org/wiki/स्टीव_जॉब्स\" \n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13655c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of extracted text : \n",
      "स्टीवन पॉल \"स्टीव\" जॉब्स (अंग्रेज़ी: Steven Paul \"Steve\" Jobs) (जन्म: २४ फरवरी, १९५५ - अक्टूबर ५, २०११) एक अमेरिकी बिजनेस टाईकून और आविष्कारक थे। वे एप्पल इंक के सह-संस्थापक और मुख्य कार्यकारी अधिकारी थे। अगस्त २०११ में उन्होने इस पद से त्यागपत्र दे दिया। जॉब्स पिक्सर एनीमेशन स्टूडियोज के मुख्य कार्यकारी अधिकारी भी रहे। सन् २००६ में वह दि वाल्ट डिज्नी कम्पनी के निदेशक मंडल के सदस्य भी रहे, जिसके बाद डिज्नी ने पिक्सर का अधिग्रहण कर लिया था। १९९५ में आई फिल्म टॉय स्टोरी के  वह  कार्यकारी निर्माता \n"
     ]
    }
   ],
   "source": [
    "# Extract all paragraph text\n",
    "text = ' '.join(p.get_text() for p in soup.select('p'))\n",
    "print(\"Preview of extracted text : \")\n",
    "print(text[:500])    # Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f7d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dai.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Tokenization using NLTK\n",
    "# -----------------------\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a4d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NLTK Sentence Count: 11\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "nltk_sentence = sent_tokenize(text)\n",
    "print(f\"\\n NLTK Sentence Count: {len(nltk_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4709d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Count  : 1834\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "nltk_words = word_tokenize(text)\n",
    "print(f'NLTK Word Count  : {len(nltk_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9414cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xx-ent-wiki-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.8.0/xx_ent_wiki_sm-3.8.0-py3-none-any.whl (11.1 MB)\n",
      "     ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "     --------------------------------------  11.0/11.1 MB 69.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.1/11.1 MB 37.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xx-ent-wiki-sm\n",
      "Successfully installed xx-ent-wiki-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('xx_ent_wiki_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd72ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1ec60ff4910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Tokenization using spaCy\n",
    "# ------------------------\n",
    "import spacy\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# Add sentencizer since xx_ent_wiki_sm doesn't include parser\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1bb0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Spacy Sentence Count : 88\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Sentence tokenization\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "print(f\"\\n Spacy Sentence Count : {len(spacy_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf16e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Spacy Word Count : 1952\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "spacy_words = [token.text for token in doc]\n",
    "print(f\"\\n Spacy Word Count : {len(spacy_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6923eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's sent_tokenize uses a Punkt model trained primarily on Enfglish.\n",
    "# it does not handle hindi sentences boundaries ( like  | ) very well, which is why it returned only 11 sentences.\n",
    "\n",
    "# spaCy (with xx_ent_wiki_sm + sentencizer) handles Unicode-aware punctuation better, including hindi sentences boundaries ( | ), leading to more accurate sentence splits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
